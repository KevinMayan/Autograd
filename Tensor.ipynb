{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b6a8ce0-6f5e-443d-8e47-91569fa93b67",
   "metadata": {},
   "source": [
    "## Importando Utilit√°rios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "2ebbb0c9-3845-4043-95fa-17a095ce5c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e511788c-0861-4f63-8c6b-d725b3fc6ce1",
   "metadata": {},
   "source": [
    "## Criando Classe Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "b719c3e5-b723-4ee1-a1a8-05e56b8b68df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tensor():\n",
    "\n",
    "    def __init__(self, value, requires_grad=False):\n",
    "\n",
    "        self.value = np.array(value, dtype=np.float64)\n",
    "\n",
    "        self.requires_grad = requires_grad\n",
    "\n",
    "        self.grad_fn = None\n",
    "\n",
    "        self.grad = None\n",
    "\n",
    "        self.children = []\n",
    "\n",
    "    def __add__(self, other):\n",
    "\n",
    "        result = Tensor(self.value + other.value)\n",
    "\n",
    "        def add_backward():\n",
    "\n",
    "            if result.grad is None:\n",
    "\n",
    "                return RuntimeError(\"result.grad is None.\")\n",
    "\n",
    "            if self.grad is None:\n",
    "\n",
    "                self.grad = (result.grad.copy())\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.grad += (result.grad.copy())\n",
    "\n",
    "            if other.grad is None:\n",
    "\n",
    "                other.grad = (result.grad.copy())\n",
    "\n",
    "            else:\n",
    "\n",
    "                other.grad += (result.grad.copy())\n",
    "        \n",
    "        if self.requires_grad or other.requires_grad:\n",
    "\n",
    "            result.requires_grad = True\n",
    "\n",
    "            result.grad_fn = add_backward\n",
    "\n",
    "            if self.requires_grad:\n",
    "    \n",
    "                result.children.append(self)\n",
    "    \n",
    "            if other.requires_grad:\n",
    "    \n",
    "                result.children.append(other)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __sub__(self, other):\n",
    "\n",
    "        result = Tensor(self.value - other.value)\n",
    "\n",
    "        def sub_backward():\n",
    "\n",
    "            if result.grad is None:\n",
    "\n",
    "                return RuntimeError(\"result.grad is None.\")\n",
    "\n",
    "            if self.grad is None:\n",
    "\n",
    "                self.grad = (result.grad.copy())\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.grad += (result.grad.copy())\n",
    "\n",
    "            if other.grad is None:\n",
    "\n",
    "                other.grad = (-result.grad.copy())\n",
    "\n",
    "            else:\n",
    "\n",
    "                other.grad -= (result.grad.copy())\n",
    "\n",
    "        \n",
    "        if self.requires_grad or other.requires_grad:\n",
    "\n",
    "            result.requires_grad = True\n",
    "\n",
    "            result.grad_fn = sub_backward\n",
    "\n",
    "            if self.requires_grad:\n",
    "    \n",
    "                result.children.append(self)\n",
    "    \n",
    "            if other.requires_grad:\n",
    "    \n",
    "                result.children.append(other)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __mul__(self, other):\n",
    "\n",
    "        result = Tensor(self.value * other.value)\n",
    "\n",
    "        def mul_backward():\n",
    "\n",
    "            if result.grad is None:\n",
    "\n",
    "                return RuntimeError(\"result.grad is None.\")\n",
    "\n",
    "            if self.grad is None:\n",
    "\n",
    "                self.grad = (result.grad.copy() * other.value)\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.grad += (result.grad.copy() * other.value)\n",
    "\n",
    "            if other.grad is None:\n",
    "\n",
    "                other.grad = (result.grad.copy() * self.value)\n",
    "\n",
    "            else:\n",
    "\n",
    "                other.grad += (result.grad.copy() * self.value)\n",
    "\n",
    "        \n",
    "        if self.requires_grad or other.requires_grad:\n",
    "\n",
    "            result.requires_grad = True\n",
    "\n",
    "            result.grad_fn = mul_backward\n",
    "\n",
    "            if self.requires_grad:\n",
    "    \n",
    "                result.children.append(self)\n",
    "    \n",
    "            if other.requires_grad:\n",
    "    \n",
    "                result.children.append(other)\n",
    "\n",
    "        return result\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "\n",
    "        result = Tensor(self.value / other.value)\n",
    "\n",
    "        def div_backward():\n",
    "\n",
    "            if result.grad is None:\n",
    "\n",
    "                return RuntimeError(\"result.grad is None.\")\n",
    "\n",
    "            if self.grad is None:\n",
    "\n",
    "                self.grad = (result.grad.copy() * (np.float64(1.0) / other.value.copy()))\n",
    "\n",
    "            else:\n",
    "\n",
    "                self.grad += (result.grad.copy() * (np.float64(1.0) / other.value.copy()))\n",
    "\n",
    "            if other.grad is None:\n",
    "\n",
    "                other.grad = (result.grad.copy() * (-self.value.copy() * (other.value.copy() ** np.float64(-2.0))))\n",
    "\n",
    "            else:\n",
    "\n",
    "                other.grad += (result.grad.copy() * (-self.value.copy() * (other.value.copy() ** np.float64(-2.0))))\n",
    "\n",
    "        \n",
    "        if self.requires_grad or other.requires_grad:\n",
    "\n",
    "            result.requires_grad = True\n",
    "\n",
    "            result.grad_fn = div_backward\n",
    "\n",
    "            if self.requires_grad:\n",
    "    \n",
    "                result.children.append(self)\n",
    "    \n",
    "            if other.requires_grad:\n",
    "    \n",
    "                result.children.append(other)\n",
    "\n",
    "        return result\n",
    "        \n",
    "        \n",
    "    def backward(self):\n",
    "\n",
    "        if self.requires_grad is None:\n",
    "\n",
    "            return RuntimeError(\"requires_grad is set to False on that Tensor.\")\n",
    "\n",
    "        if self.grad is None:\n",
    "\n",
    "            self.grad = np.ones_like(self.value)\n",
    "\n",
    "        if self.grad_fn is not None:\n",
    "\n",
    "            self.grad_fn()\n",
    "        \n",
    "        for child in self.children:\n",
    "\n",
    "            child.backward()\n",
    "\n",
    "    def __str__(self):\n",
    "\n",
    "        return \"Tensor({}, requires_grad={}, grad_fn={})\".format(self.value, self.requires_grad, self.grad_fn.__name__ if self.grad_fn != None else None)\n",
    "\n",
    "    def __repr__(self):\n",
    "\n",
    "        return \"Tensor({}, requires_grad={}, grad_fn={})\".format(self.value, self.requires_grad, self.grad_fn.__name__ if self.grad_fn != None else None)       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91915d27-e340-4df8-8968-4fbc872f3b24",
   "metadata": {},
   "source": [
    "## Computando Gradientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "bca56d82-c5a6-48f9-a095-2c2d04427496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.51851852, 0.8       , 1.14074074]),\n",
       " array([-0.20740741, -0.4       , -0.65185185]),\n",
       " array([1., 1., 1.]))"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Tensor([20.0,30.0,40.0], requires_grad=True)\n",
    "\n",
    "b = Tensor([50.0, 60.0, 70.0], requires_grad=True)\n",
    "\n",
    "c = ((a + b) / (a - b)) * ((a + b) / (a - b))\n",
    "\n",
    "c.backward()\n",
    "\n",
    "a.grad, b.grad, c.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
